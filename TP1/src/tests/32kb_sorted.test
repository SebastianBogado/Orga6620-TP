

































    A blob object is the content of a file. Blob objects have no file name, time stamps, or other metadata.
    Aborting operations or backing out changes will leave useless dangling objects in the database. These are generally a small fraction of the continuously growing history of wanted objects. Git will automatically perform garbage collection when enough loose objects have been created in the repository. Garbage collection can be called explicitly using git gc --prune.[30]
    A commit object links tree objects together into a history. It contains the name of a tree object (of the top-level source directory), a time stamp, a log message, and the names of zero or more parent commit objects.
Another property of Git is that it snapshots directory trees of files. The earliest systems for tracking versions of source code, SCCS and RCS, worked on individual files and emphasized the space savings to be gained from interleaved deltas (SCCS) or delta encoding (RCS) the (mostly similar) versions. Later revision control systems maintained this notion of a file having an identity across multiple revisions of a project. However, Torvalds rejected this concept.[31] Consequently, Git does not explicitly record file revision relationships at any level below the source code tree.
    As part of its toolkit design, Git has a well-defined model of an incomplete merge, and it has multiple algorithms for completing it, culminating in telling the user that it is unable to complete the merge automatically and manual editing is required.
    A tag object is a container that contains reference to another object and can hold additional meta-data related to another object. Most commonly, it is used to store a digital signature of a commit object corresponding to a particular release of the data being tracked by Git.
    A tree object is the equivalent of a directory. It contains a list of file names, each with some type bits and the name of a blob or tree object that is that file, symbolic link, or directory's contents. This object describes a snapshot of the source tree.
        BitKeeper was not only the first source control system that I ever felt was worth using at all, it was also the source control system that taught me why there's a point to them, and how you actually can do things. So Git in many ways, even though from a technical angle it is very very different from BitKeeper (which was another design goal, because I wanted to make it clear that it wasn't a BitKeeper clone), a lot of the flows we use with Git come directly from the flows we learned from BitKeeper.[10]
Characteristics
Compatibility with existing systems/protocols
Cryptographic authentication of history
Data structures
Distributed development
Each object is identified by a SHA-1 hash of its contents. Git computes the hash, and uses this value for the object's name. The object is put into a directory mat
Efficient handling of large projects
        For the first 10 years of kernel maintenance, we literally used tarballs and patches, which is a much superior source control management system than CVS is, but I did end up using CVS for 7 years at a commercial company [Transmeta[9]] and I hate it with a passion. When I say I hate CVS with a passion, I have to also say that if there are any SVN [Subversion] users in the audience, you might want to leave. Because my hatred of CVS has meant that I see Subversion as being the most pointless project ever started. The slogan of Subversion for a while was "CVS done right", or something like that, and if you start with that kind of slogan, there's nowhere you can go. There is no way to do CVS right.[10]
From this initial design approach, Git has developed the full set of features expected of a traditional SCM,[22] with features mostly being created as needed, then refined and extended over time.
Garbage accumulates unless collected
Git development began after many Linux kernel developers chose to give up access to BitKeeper, a proprietary SCM system that had previously been used to maintain the project.[7] The copyright holder of BitKeeper, Larry McVoy, had withdrawn free use of the product after he claimed that Andrew Tridgell had reverse-engineered the BitKeeper protocols.
Git has two data structures: a mutable index that caches information about the working directory and the next revision to be committed; and an immutable, append-only object database.
Git implements several merging strategies; a non-default can be selected at merge time:[37]
Git's design is a synthesis of Torvalds's experience with Linux in maintaining a large distributed development project, along with his intimate knowledge of file system performance gained from the same project and the urgent need to produce a working system in short order. These influences led to the following implementation choices:
Git's design was inspired by BitKeeper and Monotone.[20][21] Git was originally designed as a low-level version control system engine on top of which others could write front ends, such as Cogito or StGIT.[21] However, the core Git project has since become a complete revision control system that is usable directly.[22]
Git (software)
Git's primitives are not inherently a source code management (SCM) system. Torvalds explains,[39]
    Git stores each newly created object as a separate file. Although individually compressed, this takes a great deal of space and is inefficient. This is solved by the use of "packs" that store a large number of objects in a single file (or network byte stream) called "packfile", delta-compressed among themselves. Packs are compressed using the heuristic that files with the same name are probably similar, but do not depend on it for correctness. A corresponding index file is created for each packfile, telling the offset of each object in the packfile. Newly created objects (newly added history) are still stored singly, and periodic repacking is required to maintain space efficiency. The process of packing the repository can be very computationally expensive. By allowing objects to exist in the repository in a loose, but quickly generated format, git allows the expensive pack operation to be deferred until later when time does not matter (e.g. the end of the work day). Git does periodic repacking automatically but manual repacking is also possible with the git gc command. For data integrity, both packfile and its index have SHA-1 checksum inside, and also the file name of packfile contains a SHA-1 checksum. To check integrity, run the git fsck command.
    Git supports rapid branching and merging, and includes specific tools for visualizing and navigating a non-linear development history. A core assumption in Git is that a change will be merged more often than it is written, as it is passed around various reviewers. Branches in git are very lightweight: A branch in git is only a reference to a single commit. With its parental commits, the full branch structure can be constructed.
    Git was designed as a set of programs written in C, and a number of shell scripts that provide wrappers around those programs.[28] Although most of those scripts have since been rewritten in C for speed and portability, the design remains, and it is easy to chain the components together.[29]
History
    However, the SCMs I've looked at make this hard. One of the things (the main thing, in fact) I've been working at is to make that process really efficient. If it takes half a minute to apply a patch and remember the changeset boundary etc. (and quite frankly, that's fast for most SCMs around for a project the size of Linux), then a series of 250 emails (which is not unheard of at all when I sync with Andrew, for example) takes two hours. If one of the patches in the middle doesn't apply, things are bad bad bad.
    (If a patch apply takes three seconds, even a big series of patches is not a problem: if I get notified within a minute or two that it failed half-way, that's fine, I can then just fix it up manually. That's why latency is critical—if I'd have to do things effectively "offline", I'd by definition not be able to fix it up when problems happen).
ign
Implicit revision relationships have some significant consequences:
    In many ways you can just see git as a filesystem — it's content-addressable, and it has a notion of versioning, but I really really designed it coming at the problem from the viewpoint of a filesystem person (hey, kernels is what I do), and I actually have absolutely zero interest in creating a traditional SCM system.
In software development, Git (/ɡɪt/) is a distributed revision control and source code management (SCM) system with an emphasis on speed.[3] Git was initially designed and developed by Linus Torvalds for Linux kernel development; it has since been adopted by many other projects. Every Git working directory is a full-fledged repository with complete history and full revision tracking capabilities, not dependent on network access or a central server. Git is free software distributed under the terms of the GNU General Public License version 2.
    It is slightly more expensive to examine the change history of a single file than the whole project.[32] To obtain a history of changes affecting a given file, Git must walk the global history and then determine whether each change modified that file. This method of examining history does, however, let Git produce with equal efficiency a single history showing the changes to an arbitrary set of files. For example, a subdirectory of the source tree plus an associated global header file is a very common case.
    Like Darcs, BitKeeper, Mercurial, SVK, Bazaar and Monotone, Git gives each developer a local copy of the entire development history, and changes are copied from one such repository to another. These changes are imported as additional development branches, and can be merged in the same way as a locally developed branch.
Linus Torvalds has quipped about the name "git", which is English slang for a stupid or unpleasant person. Torvalds said: "I'm an egotistical bastard, and I name all my projects after myself. First 'Linux', now 'git'."[4][5] The man page describes git as "the stupid content tracker".[6]
Name
    Now, BK wasn't a speed demon either (actually, compared to everything else, BK is a speed demon, often by one or two orders of magnitude), and took about 10–15 seconds per email when I merged with Andrew. HOWEVER, with BK that wasn't as big of an issue, since the BK<->BK merges were so easy, so I never had the slow email merges with any of the other main developers. So a patch-application-based SCM "merger" actually would need to be faster than BK is. Which is really really really hard.
    octopus: This is the default when merging more than two heads.
Periodic explicit object packing
Pluggable merge strategies
    recursive: This is the default when pulling or merging one branch, and is a variant of the three-way merge algorithm. "When there are more than one common ancestors that can be used for three-way merge, it creates a merged tree of the common ancestors and uses that as the reference tree for the three-way merge. This has been reported to result in fewer merge conflicts without causing mis-merges by tests done on actual merge commits taken from Linux 2.6 kernel development history. Additionally this can detect and handle merges involving renames."[38]
    Renames are handled implicitly rather than explicitly. A common complaint with CVS is that it uses the name of a file to identify its revision history, so moving or renaming a file is not possible without either interrupting its history, or renaming the history and thereby making the history inaccurate. Most post-CVS revision control systems solve this by giving a file a unique long-lived name (a sort of inode number) that survives renaming. Git does not record such an identifier, and this is claimed as an advantage.[33][34] Source code files are sometimes split or merged as well as simply renamed,[35] and recording this as a simple rename would freeze an inaccurate description of what happened in the (immutable) history. Git addresses the issue by detecting renames while browsing the history of snapshots rather than recording it when making the snapshot.[36] (Briefly, given a file in revision N, a file of the same name in revision N−1 is its default ancestor. However, when there is no like-named file in revision N−1, Git searches for a file that existed only in revision N−1 and is very similar to the new file.) However, it does require more CPU-intensive work every time history is reviewed, and a number of options to adjust the heuristics.
    Repositories can be published via HTTP, FTP, rsync, or a Git protocol over either a plain socket or ssh. Git also has a CVS server emulation, which enables the use of existing CVS clients and IDE plugins to access Git repositories. Subversion and svk repositories can be used directly with git-svn.
    resolve: the traditional three-way merge algorithm.
    So I'm writing some scripts to try to track things a whole lot faster. Initial indications are that I should be able to do it almost as quickly as I can just apply the patch, but quite frankly, I'm at most half done, and if I hit a snag maybe that's not true at all. Anyway, the reason I can do it quickly is that my scripts will not be an SCM, they'll be a very specific "log Linus' state" kind of thing. That will make the linear patch merge a lot more time-efficient, and thus possible.
Some data flows and storage levels in the Git revision control system.
Strong support for non-linear development
    Support a distributed, BitKeeper-like workflow:
    Take CVS as an example of what not to do; if in doubt, make the exact opposite decision. To quote Torvalds, speaking somewhat tongue-in-cheek:
The development of Git began on 3 April 2005.[12] The project was announced on 6 April,[13] and became self-hosting as of 7 April.[12] The first merge of multiple branches was done on 18 April.[14] Torvalds achieved his performance goals; on 29 April, the nascent Git was benchmarked recording patches to the Linux kernel tree at the rate of 6.7 per second.[15] On 16 June, the kernel 2.6.12 release was managed by Git.[16]
The first three criteria eliminated every pre-existing version control system, except for Monotone, and the fourth excluded everything.[10] So, immediately after the 2.6.12-rc2 Linux kernel development release,[10] he set out to write his own.[10]
    The Git history is stored in such a way that the id of a particular revision (a "commit" in Git terms) depends upon the complete development history leading up to that commit. Once it is published, it is not possible to change the old versions without it being noticed. The structure is similar to a hash tree, but with additional data at the nodes as well as the leaves.[27] (Mercurial and Monotone also have this property.)
The index serves as connection point between the object database and the working tree.
The object database contains four types of objects:
Toolkit-based design
Torvalds had several design criteria:
    Torvalds has described Git as being very fast and scalable,[23] and performance tests done by Mozilla showed it was an order of magnitude faster than some revision control systems, and fetching revision history from a locally stored repository can be one hundred times faster than fetching it from the remote server.[24][25] In particular, Git does not get slower as the project history grows larger.[26]
Torvalds wanted a distributed system that he could use like BitKeeper, but none of the available free systems met his needs, particularly his performance needs. From an email he wrote on 7 April 2005 while writing the first prototype:[8]
    Very high performance.
    Very strong safeguards against corruption, either accidental or malicious.[10][11]
While strongly influenced by BitKeeper, Torvalds deliberately attempted to avoid conventional approaches, leading to a unique design.[17] He developed the system until it was usable by technical users, then turned over maintenance on 26 July 2005 to Junio Hamano, a major contributor to the project.[18] Hamano was responsible for the 1.0 release on 21 December 2005,[19] and remains the project's maintainer.






































   1. ^ Hamano, Junio (2012-08-20). "[ANNOUNCE] Git 1.7.12". kernel mailing list. Retrieved 2012-08-20.
   2. ^ "git/git.git/tree". git.kernel.org. Retrieved 2009-06-15.
   3. ^ Linus Torvalds (2005-04-07). "Re: Kernel SCM saga..". linux-kernel mailing list. "So I'm writing some scripts to try to track things a whole lot f
        a branch is a special mobile tag that is updated whenever a commit is made to that branch.
            a branch is thus really a label that is always on the commit that most SCMs would call the branch/HEAD version.
Adoption
After the first "git clone" or "git init" operation, git sets the "current branch" to be "master" and will make sure that the working tree reflects the commit to which the "master" branch points. Each commit will have its parent set to the commit currently marked "master", and "master" will be moved to that new commit, ready for the next one.
A Git repository — data and metadata — is completely contained within its directory, so a normal system-level copy (or rename, or delete) of an entire Git repository is a safe operation. The resulting copy is both independent of and unaware of the original.
        a tag marks a particular commit and is only ever moved manually; other SCMs might call it a label.
    BerliOS
    Bitbucket
Branches and tags
    Branches and tags are very cheap and almost identical — they can be made whenever one wants to remember a particular point in development.
            branches from other repositories are maintained in .git/refs/remotes
ching the first two characters of its hash. The rest of the hash is used as the file name for that object.
    CodePlex[54]
            collectively, all the commits form a tree-like structure based on the parent relationship, whence the concept of branches derives.
Commits
    commits, representing checked-in versions of files and directories live in .git/objects
    Comparison of open source software hosting facilities
    Comparison of revision control software
Configuration
            creating a commit does not affect the working tree.
        creating a commit (with "git commit") copies the staged changes into a new commit and then clears the index
Data
    Distributed revision control
During a normal work session, one can choose to ask Git to note for later some or all of the differences between how the files are now and how Git's permanent repository remembers them. The place it remembers them is called "the staging area" (or index) and remembering them is called "staging". One can also remove or exclude selected changes from staging if it suits. Staging diffs is easy, and is similar in concept to creating a patch file. Staging is the precursor to committing those changes as a permanent record in the Git repository.
        each commit has a reference to its parent commit(s).
        each commit is uniquely identified to Git commands by a 40-character identifier, but it's usually sufficiently unique to provide only the first seven characters of it.
        each commit represents a snapshot of the full tree of files current at the point the commit was created.
        file changes explicitly added to the index are termed "staged changes".
        file changes made but not (or not yet) added to the index are termed "unstaged changes".
    Gerrit (software)
    GitHub
.gitignore
Git is primarily developed on GNU/Linux, although it also supports most major operating systems including BSD, Solaris, OS X, and Microsoft Windows.[41]
    Gitorious
        .git/refs/heads/master contains a 40 character identifier for the commit that is the master branch. Much Git metadata is in plain text files.
Git servers typically listen on TCP port 9418.[40]
Git stores each revision of a file as a unique blob object. The relationships between the blobs can be found through examining the tree and commit objects. Newly added objects are stored in their entirety using zlib compression. This can consume a large amount of disk space quickly, so objects can be combined into packs, which use delta compression to save space, storing blobs as their changes relative to other blobs.
            git will complain if a shortened identifier is ambiguous.
Global settings
    GNU Savannah
    Google Code
Hooks
    hooks are within the .git directory and not therefore under source control, i.e., not cloned from remote repositories.
Index (stage)
    information about which remote repositories this repository can talk to, how local branches relate to remote branches, etc., lives in .git/config, in plain text.
    it can refer to itself, to exclude itself from source control
    it is outside the .git directory with the "normal" files, in order to keep it under source control: everyone in a project typically needs the same exclusions.
    JavaForge (with pull requests to control source code contribution)
    List of revision control software
            local branches live in .git/refs/heads
Metadata
Portability
Portal icon 	Free software portal
References
    Repo (script)
See also
See also: Comparison of open source software hosting facilities
Source code hosting
    SourceForge
    special scripts that run for various source-control events to enforce policy.
        staged and unstaged changes coexist harmlessly (and can be seen with "git status").
Stash
Terminology and structure
The "behind the scenes" source control elements live within the .git/ directory at the root of the repository.
The Dulwich implementation of Git is a pure Python software component for Python 2.[43]
The Eclipse Foundation reported in its annual community survey that as of May 2012 Git is the primary source control system used by over 27% of professional software developers, an increase from 12.8% in 2011.[47] Open source directory Ohloh reports a similar uptake among open source projects.[48]
The files and directories — the things typically considered to be "under source control, but checked out" that the user changes in an editor or otherwise — live under the top repository directory, but outside of its .git/ directory.
The following Web sites provide free source code hosting for Git repositories:[53]
    the index — a set of file changes being collected (with "git add") to create in the repository as a single commit (version)
        the index can even be told to stage selected "hunks" (patches/blocks of change) within a file, while leaving other hunks within the very same file unstaged.
The JGit implementation of Git is a pure Java software library, designed to be embedded in any Java application.[42]
The libgit2 implementation of Git is an ANSI C software library, which can be built on multiple platforms including Microsoft Windows, Linux, Mac OS X, and BSD.[44] It is used as the basis for Git libraries for the Ruby and Python programming languages.[45][46]
            the new commit becomes the current commit and incorporates the staged changes, so they no longer show up as differences between the commit and the working tree.
    these are ways to refer to particular commits, and they live in .git/refs and .git/packed-refs
    the stash ("git stash") is a special list of commits used to hold temporary changes.
The UK IT jobs website itjobswatch.co.uk reports that as of July 2012, approximately 7% of UK permanent software development job openings list Git as a requirement,[49] compared to 19.9% for Subversion,[50] 9.1% for Microsoft Team Foundation Server,[51] and 2.6% for Visual SourceSafe.[52]
The working environment looks much like it would as if Git wasn't around: the working tree is a normal directory with only the subtle presence of a .git directory to suggest otherwise.
This is a plain text configuration file which contains filename patterns that git should consider invisible for the purposes of source control — e.g., *.o, *.class, bin/
            unstaged changes thus continue to show as differences, until added to the index (or reverted).
User preferences — real name, email address, preferences, command aliases, etc. — are held in a user's home directory in a file called ~/.gitconfig.
Working environment
Working tree
La administración tal y como la conocemos hoy, es el resultado histórico integrado de
la contribución acumulada de numerosos pioneros, algunos filósofos, otros físicos,
economistas, estadistas y otros, entre los que se incluyen empresarios que con el transcurso
del tiempo fueron desarrollando y divulgando obras y teorías en su campo de actividad.
En toda su larga historia y hasta los inicios del Siglo XX, la administración se desarrolló
con una lentitud impresionante. Sólo a partir de este siglo atravesó etapas de desarrollo de
notable pujanza e innovación. En la actualidad la sociedad de la mayor parte de los países
desarrollados en una sociedad pluralista de organizaciones, donde a mayoría de las
obligaciones sociales (como la producción, a prestación de servicios especializados de
educación o de atención hospitalaria, la garantía de la defensa nacional o la preservación del
medioambiente), es confiada a organizaciones que son administradas por grupos directivos
propios para poder ser más eficaces.
A diferencia de lo anterior, a finales del siglo pasado la sociedad funcionaba de manera
completamente diferente, a pesar de que en la historia de la humanidad siempre existió el
trabajo, la historia de las organizaciones y de su administración, es un capítulo que comenzó
en épocas recientes.
En el transcurso de la historia de la humanidad siempre existió alguna forma
rudimentaria de administrar las organizaciones, desde las más simples hasta las más
complejas. La Revolución Industrial dio lugar al contexto industrial, tecnológico, social, político
y económico de las situaciones. Problemas y variables a partir de los cuales se iniciaría la
teoría clásica de la administración.
La influencia de los pioneros y de los empresarios fue fundamental para la creación de
las condiciones básicas que motivaron el surgimiento de las teorías administrativas.

La administración tal y como la conocemos hoy, es el resultado histórico integrado de
la contribución acumulada de numerosos pioneros, algunos filósofos, otros físicos,
economistas, estadistas y otros, entre los que se incluyen empresarios que con el transcurso
del tiempo fueron desarrollando y divulgando obras y teorías en su campo de actividad.
En toda su larga historia y hasta los inicios del Siglo XX, la administración se desarrolló
con una lentitud impresionante. Sólo a partir de este siglo atravesó etapas de desarrollo de
notable pujanza e innovación. En la actualidad la sociedad de la mayor parte de los países
desarrollados en una sociedad pluralista de organizaciones, donde a mayoría de las
obligaciones sociales (como la producción, a prestación de servicios especializados de
educación o de atención hospitalaria, la garantía de la defensa nacional o la preservación del
medioambiente), es confiada a organizaciones que son administradas por grupos directivos
propios para poder ser más eficaces.
A diferencia de lo anterior, a finales del siglo pasado la sociedad funcionaba de manera
completamente diferente, a pesar de que en la historia de la humanidad siempre existió el
trabajo, la historia de las organizaciones y de su administración, es un capítulo que comenzó
en épocas recientes.
En el transcurso de la historia de la humanidad siempre existió alguna forma
rudimentaria de administrar las organizaciones, desde las más simples hasta las más
complejas. La Revolución Industrial dio lugar al contexto industrial, tecnológico, social, político
y económico de las situaciones. Problemas y variables a partir de los cuales se iniciaría la
teoría clásica de la administración.
La influencia de los pioneros y de los empresarios fue fundamental para la creación de
las condiciones básicas que motivaron el surgimiento de las teorías administrativas.
EVOLUCIÓN DE LA TEORÍA ADMINISTRATIVA.
ESCUELA CLÁSICA
La Escuela Clásica es la que se ocupa del aumento de la eficiencia de la empresa a
través de su organización y de la aplicación de principios científicos generales de la
administración.
A comienzos del presente siglo, dos ingenieros desarrollaron los trabajos pioneros de la
administración. Uno es el estadounidense Frederick Winslow Taylor, quien desarrollo la
llamada escuela de la administración científica, escuela que se preocupa por aumentar la
eficiencia e la industria a través, inicialmente, de la racionalización el trabajo del obrero. El otro
es el europeo Henri Fayol, quien desarrollo la llamada teoría clásica. Aunque los dos autores
no se comunicaron entre sí y partieron de puntos de vistas diferentes y aun opuestos, lo cierto
es que sus ideas constituyen las bases del llamado enfoque clásico o tradicional de la
administración cuyos postulados dominaron casi las cuatro primeras décadas de este siglo en
el panorama administrativo de las organizaciones.
De este modo, en general, el enfoque clásico de la administración se constituyo por:
1.
De una parte, la escuela de administración científica desarrollada en los Estados
Unidos a partir de los trabajos de Taylor. Esta escuela estaba constituida
principalmente por ingenieros, como Frederick Winslow Taylor (1856–1915), Henry
Lawrence Gantt (1861–1919), Frank Bunker Gilbreth (1868–1924), Harrington Emerson
(1853–1931), y otros. Se suele incluir entre ellos a Henry Ford (1863–1947), por haber
aplicado los principio de esta teoría en sus negocios.
La preocupación básica era aumentar la productividad e la empresa mediante el
aumento de la eficiencia en el nivel operacional, esto es, en el nivel de los obreros. De
allí, el énfasis en el análisis y la división del trabajo del obrero u operario, toda vez que
las tareas del cargo y e quien lo desempeñase, constituyen la unidad fundamental de la
organización. En este sentido la administración científica desarrolla un enfoque de
abajo hacia arriba (del obrero hacia el supervisor y el gerente) y de las partes (obreros
y sus cargos) hacia el todo (organización empresarial). La atención se centra en el
trabajo, en los movimiento necesarios para la ejecución de las tareas, en el tiempo
estándar determinado para su la ejecución.
2.
De otro lado, la corriente de los anatomistas y fisiologistas de la organización,
desarrollada en Francia con los trabajos pioneros de Fayol. Esta escuela esta formada
principalmente por ejecutivos de la época principal por: Henri Fayol (1841-1925),
James J Moooney, Lyndall f, Urwick (n. 1891), Luther Gulick y otros. Esta corriente
cuya preocupación básica es aumentar la eficiencia de la empresa, a través de la forma
y disposición de los órganos competentes de la organización (departamentos) y de sus
interrelaciones estructurales se denomina teoría clásica. De allí, el énfasis en la
anatomía (estructura) y en la fisiología (funcionamiento) de la organización. En este
sentido, desarrolla un enfoque inverso a la administración científica: de arriba hacia
abajo (de la dirección hacia la ejecución), del todo (organización) hacia sus partes
componentes (departamentos). En síntesis y en visión global permitía una mejor
manera de subdividir la empresa, centralizando la dirección en un jefe principal. Fue
una corriente eminentemente teórica.
Si la administración científica se caracterizaba por el énfasis en la tarea que realiza el
obrero, la teoría clásica se caracteriza por el énfasis en la estructura que una organización
debe tener para lograr la eficiencia. En realidad el objetivo de ambas teorías era el mismo: la
búsqueda de la eficiencia de las organizaciones. Según la administración científica, esa
eficiencia se alcanza a través de la racionalización del trabajo de obrero y de la sumatoria de la
eficiencia individual.
En la teoría clásica por el contrario se parte de un todo organizacional y de su
estructura para garantizar la eficiencia en todas las partes involucradas, sean ellas órganos
(secciones, departamentos, etc)., o personas (ocupantes de cargos o ejecutores de tareas). El
microenfoque individual de cada obrero con relación a la tarea se amplia enormemente en la


